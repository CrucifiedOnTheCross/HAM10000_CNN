Конечно, вот дополненная версия технического задания с учетом вашего уточнения.

---

### **Техническое задание на разработку и обучение модели классификации дерматоскопических изображений**

**1. Общие положения**

1.1. **Наименование проекта:** Разработка и обучение сверточной нейронной сети на архитектуре DenseNet для классификации 7 типов кожных новообразований на датасете HAM10000.
1.2. **Цель проекта:** Создать программное решение на базе TensorFlow, способное с высокой точностью классифицировать дерматоскопические изображения. Модель должна быть обучена с учетом сильного дисбаланса классов в исходных данных и протестирована в рамках нескольких сценариев обучения.
1.3. **Стек технологий:** Python, TensorFlow, Keras, Pandas, Scikit-learn, Matplotlib, Seaborn.

**2. Требования к данным и их обработке**

2.1. **Источник данных:** Датасет HAM10000 ("Human Against Machine with 10000 training images").
2.2. **Классы для распознавания:** Модель должна классифицировать все 7 классов, представленных в датасете:
    *   Актинический кератоз и интраэпителиальная карцинома (akiec)
    *   Базально-клеточная карцинома (bcc)
    *   Доброкачественные кератоподобные образования (bkl)
    *   Дерматофиброма (df)
    *   Меланома (mel)
    *   Меланоцитарный невус (nv)
    *   Сосудистые поражения (vasc)

2.3. **Класс для работы с данными:** Должен быть реализован класс `DatasetLoader`, отвечающий за следующие функции:
    *   Загрузка метаданных и изображений из указанной директории.
    *   Предварительная обработка изображений: изменение размера до единого стандарта (например, 224x224 пикселя), нормализация значений пикселей.
    *   Разделение данных на обучающую, валидационную и тестовую выборки.
    *   Применение техник аугментации к обучающей выборке.
    *   Подготовка данных в формате `tf.data.Dataset` для эффективной подачи в модель.

2.4. **Дополнительные сценарии обработки данных:**
    *   **Аугментация датасета:** Для борьбы с переобучением и увеличения разнообразия обучающих данных необходимо применять следующие аугментации:
        *   Случайные повороты изображений.
        *   Случайные горизонтальные и вертикальные отражения.
        *   Случайное изменение масштаба (zoom).
        *   Случайное изменение яркости и контрастности.
        *   Добавление гауссова шума для повышения робастности модели.
    *   **Борьба с дисбалансом классов:** Датасет HAM10000 является сильно несбалансированным. Для решения этой проблемы необходимо реализовать:
        *   **Сценарий со штрафами:** Вычисление и применение весов для классов (class weights) обратно пропорционально их частоте в обучающей выборке. Эти веса должны передаваться в функцию `model.fit()` для штрафа за ошибки на минорных классах.

**3. Требования к архитектуре модели и обучению**

3.1. **Базовая архитектура:** В качестве основы используется сверточная нейронная сеть DenseNet (рекомендуется DenseNet121 или DenseNet201) с весами, предобученными на датасете ImageNet.
3.2. **"Голова" модели:** Поверх базовой модели DenseNet должна быть добавлена кастомная классификационная "голова", состоящая из:
    *   Слоя глобального усреднения (GlobalAveragePooling2D).
    *   Одного или нескольких полносвязных слоев (Dense) с функцией активации ReLU.
    *   Слоя Dropout для регуляризации.
    *   Выходного полносвязного слоя с 7 нейронами (по числу классов) и функцией активации Softmax.

3.3. **Основные сценарии обучения (Transfer Learning):** Необходимо реализовать возможность запуска обучения по трем ключевым сценариям:
    *   **Сценарий 1: Обучение "головы".** Все слои базовой модели DenseNet замораживаются (`trainable = False`), и обучение проходит только для кастомной "головы".
    *   **Сценарий 2: Частичное размораживание.** Замораживается определенный процент нижних слоев базовой модели, а обучение ("fine-tuning") происходит для верхних слоев и "головы". Должна быть возможность задавать процент размораживаемых слоев (например, обучать верхние 20% слоев).
    *   **Сценарий 3: Полное обучение.** Обучаются все слои нейронной сети (и база, и "голова"). Этот сценарий должен запускаться с низким темпом обучения (learning rate).

**4. Требования к среде выполнения и запуску**

4.1. **Серверное исполнение:** Разработка должна вестись с учетом того, что итоговое обучение и эксплуатация модели будут производиться на удаленном сервере (например, под управлением ОС Linux). **Локальный запуск на персональных машинах не предусматривается.**
4.2. **Запуск из командной строки:** Программное решение должно быть реализовано в виде скриптов, запускаемых из командной строки. Все ключевые параметры, включая выбор сценария обучения, аугментации, использования весов классов, путей к данным и директории для сохранения результатов, должны передаваться через аргументы командной строки (например, с использованием библиотеки `argparse`). Решение не должно требовать графического интерфейса или интерактивного вмешательства пользователя в процессе выполнения.
4.3. **Поддержка нескольких GPU:** Система должна быть рассчитана на использование двух видеокарт. Для этого необходимо использовать стратегию распределенного обучения TensorFlow `tf.distribute.MirroredStrategy` для обеспечения синхронного обучения модели на всех доступных GPU.

**5. Требования к метрикам и оценке качества**

5.1. **Ключевые метрики:** Ввиду сильного дисбаланса классов, точность (Accuracy) не является достаточной метрикой. В ходе обучения и тестирования необходимо отслеживать и логировать следующие метрики для каждого класса и в среднем:
    *   **Точность (Precision)**
    *   **Полнота (Recall / Sensitivity)**: критически важная метрика для медицинских задач.
    *   **F1-мера (F1-score)**
    *   **Сбалансированная точность (Balanced Accuracy)**
    *   **Площадь под ROC-кривой (AUC-ROC)**
    *   **Специфичность (Specificity)**

5.2. **Визуализация результатов:** По итогам обучения для тестовой выборки должна строиться и сохраняться **матрица ошибок (Confusion Matrix)**.

**6. Требования к логированию и организации экспериментов**

6.1. **Логирование:** Весь процесс обучения (эпохи, значения функции потерь и метрик на обучающей и валидационной выборках) должен логироваться в текстовый файл. Записи лога должны содержать временные метки.
6.2. **Структура хранения результатов:** Каждый запуск эксперимента должен сохраняться в отдельную директорию. Имя директории должно формироваться автоматически по шаблону:
    `<ИМЯ_СЦЕНАРИЯ_ОБУЧЕНИЯ>_<ИМЯ_ДОП_СЦЕНАРИЯ>_<ГГГГММДД_ЧЧММСС>`

    *Примеры:*
    *   `HEAD_TRAINING_WITH_WEIGHTS_20251015_160000`
    *   `FULL_TRAINING_AUGMENTATION_20251015_183000`

6.3. **Содержимое директории эксперимента:**
    *   Файл с логами обучения (`training.log`).
    *   Сохраненные веса модели (лучшая и последняя эпохи).
    *   Графики обучения (loss, accuracy, f1-score и т.д.).
    *   Изображение матрицы ошибок (`confusion_matrix.png`).
    *   Текстовый файл с итоговыми метриками на тестовой выборке (`test_metrics.txt`).

**7. Дополнительные технические требования и уточнения**

7.1. **Получение данных:** Датасет HAM10000 должен быть скачан программно или предоставлены инструкции по его загрузке. Необходимо обеспечить автоматическую загрузку и распаковку датасета при первом запуске.

7.2. **Структура проекта:** Проект должен иметь следующую организацию директорий:
```
TransferLearning/
├── src/                    # Исходный код
│   ├── models/            # Модули для работы с моделями
│   ├── data/              # Модули для работы с данными
│   ├── utils/             # Вспомогательные утилиты
│   └── train.py           # Основной скрипт обучения
├── dataset/               # Датасет HAM10000
│   ├── images/           # Изображения
│   └── metadata/         # Метаданные
├── experiments/           # Результаты экспериментов
│   └── <experiment_dirs>/ # Директории отдельных экспериментов
├── configs/               # Конфигурационные файлы
└── README.md             # Документация проекта
```

7.3. **Параметры обучения через командную строку:** Все ключевые гиперпараметры должны настраиваться через аргументы командной строки:
    *   Learning rate (по умолчанию: 0.001 для обучения головы, 0.0001 для fine-tuning)
    *   Batch size (по умолчанию: 32)
    *   Количество эпох (по умолчанию: 50)
    *   Процент размораживаемых слоев для сценария 2 (по умолчанию: 20%)
    *   Пути к данным и директории сохранения результатов

7.4. **Гибкость запуска:** Система должна поддерживать различные режимы компиляции и запуска через командную строку:
    *   Выбор сценария обучения (head_only, partial_unfreeze, full_training)
    *   Включение/отключение аугментации данных
    *   Использование весов классов для борьбы с дисбалансом
    *   Выбор архитектуры DenseNet (DenseNet121, DenseNet201)

7.5. **Версия TensorFlow:** Использовать TensorFlow 2.x (рекомендуется последняя стабильная версия 2.15+). Requirements.txt файл не требуется - зависимости должны быть указаны в документации.

7.6. **Разделение данных:** 
    *   Пропорции разделения: 70% обучение / 15% валидация / 15% тест
    *   Обязательное использование стратифицированного разделения для сохранения пропорций классов во всех выборках
    *   Фиксация random_state для воспроизводимости результатов

7.7. **Сохранение моделей и чекпоинтов:**
    *   Формат сохранения модели: на выбор (.h5, SavedModel или оба)
    *   Обязательное сохранение промежуточных чекпоинтов во время обучения
    *   Сохранение лучшей модели по валидационной F1-мере
    *   Сохранение последней модели после завершения обучения

**8. Примеры запуска**

8.1. **Обучение только головы с аугментацией:**
```bash
python src/train.py --scenario head_only --augmentation --epochs 30 --lr 0.001 --batch_size 32
```

8.2. **Частичное размораживание с весами классов:**
```bash
python src/train.py --scenario partial_unfreeze --class_weights --unfreeze_percent 25 --epochs 50 --lr 0.0001
```

8.3. **Полное обучение:**
```bash
python src/train.py --scenario full_training --epochs 100 --lr 0.00001 --batch_size 16
```